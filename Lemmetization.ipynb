{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2r0S3DMKEopk0B1n+glWR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Klnishant/ANNImplementation/blob/main/Lemmetization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "trM66v-00TrA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjRHgqVd066L",
        "outputId": "c2dbfec7-d004-4271-cf62-36430dfa0b92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Lemmatization is a crucial step in text processing. It reduces words to their base forms.\""
      ],
      "metadata": {
        "id": "Pgl-mAOy1Ut6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokenize in to words\")\n",
        "words = word_tokenize(text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjipvgcY1caL",
        "outputId": "438b5130-4376-492e-c3ad-e0f926b7d020"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize in to words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lemmatization',\n",
              " 'is',\n",
              " 'a',\n",
              " 'crucial',\n",
              " 'step',\n",
              " 'in',\n",
              " 'text',\n",
              " 'processing',\n",
              " '.',\n",
              " 'It',\n",
              " 'reduces',\n",
              " 'words',\n",
              " 'to',\n",
              " 'their',\n",
              " 'base',\n",
              " 'forms',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "Jhqw1QG51mxc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatizing words\")\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "lemmatized_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT46n8sT1wD2",
        "outputId": "86b5bb03-7e6c-4cfd-b315-e048b9e70e40"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatizing words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lemmatization',\n",
              " 'is',\n",
              " 'a',\n",
              " 'crucial',\n",
              " 'step',\n",
              " 'in',\n",
              " 'text',\n",
              " 'processing',\n",
              " '.',\n",
              " 'It',\n",
              " 'reduces',\n",
              " 'word',\n",
              " 'to',\n",
              " 'their',\n",
              " 'base',\n",
              " 'form',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKQ2IZwk2Doy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}